[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Trackmania AI / Pedro AI",
    "section": "",
    "text": "This is the website of the Pedro AI project. It features an AI learning to play the popular racing game Trackmania. The entire process is streamed on Twitch allowing viewers to watch the AI improve over time.\nCheck out the blog for more information !"
  },
  {
    "objectID": "blog/posts/pedroai-rl/pedroai-rl.html",
    "href": "blog/posts/pedroai-rl/pedroai-rl.html",
    "title": "How PedroAI works",
    "section": "",
    "text": "Here you will learn how the deep reinforcement learning project PedroAI works. The project features an AI learning to play the popular racing game Trackmania. The entire process is streamed on Twitch allowing viewers to watch the AI improve over time.\nThis article is the second of a two-part series where we first learn about deep reinforcement learning basics before diving deeper into the specificities of the PedroAI project.\nA third article about the technical details of PedroAI is being written as you read this.\nHere we will describe the project at a high level and not go into technical details about the implementation. Let’s start with an overview of the training."
  },
  {
    "objectID": "blog/posts/pedroai-rl/pedroai-rl.html#collector",
    "href": "blog/posts/pedroai-rl/pedroai-rl.html#collector",
    "title": "How PedroAI works",
    "section": "Collector",
    "text": "Collector\nThe main role of the collector is to execute the reinforcement learning loop (RL loop) to collect experiences. An experience is composed of a state, an action, a reward, and the next state. The collector also loads Trackmania maps and cycle exploration strategies.\n\nRL loop\nAfter loading a map, the collector start to execute the RL loop. For the PedroAI project it looks like this:\n\n\n\nPedroAI collector\n\n\nIn practice, the collector peforms the following actions in sequence:\n\n\n\nRead current state (or use next state from step 6):\n\n\nScreenshot the game window for visual data\n\n\nRead telemetry data (speed, rpm, wheel angle … and many more)\n\n\nCompute reference trajectory data in the frame of reference of the car\n\n\n\n\nScore each possible action with the neural network\n\n\nPick an action\n\n\nApply the action in game\n\n\nWait 100ms, this is the default timestep\n\n\nRead the next state\n\n\nCompute the reward\n\n\nSend the sequence (state, action, reward, next state) to the replay buffer\n\n\n\n\nLoading maps\nA collector loads a new map every session, i.e., every 10 rounds. The goal is to frequently show different environments to the AI. This helps generalization and makes the AI better when confronted to unseen maps.\n\n\nExploration strategy\nThe exploration strategy changes every round across a session. These strategies have an impact on step “3) Pick an action” of the RL loop. The agent doesn’t always take the optimal action acording to its neural network. Sometimes it takes action at random. The goal is to prevent the agent from always driving the same way and let it discover new trajectories.\nPedroAI uses 3 differents strategies: Greedy, Epsilon-greedy and Rank Boltzmann exploration.\n\nGreedy: The agent always takes the optimal action according to its neural network.\nEpsilon-greedy: The agent does random exploration occasionally with probability ε and takes the optimal action most of the time with probability 1 - ε.\nRank Boltzmann exploration: The agent draws actions from a Boltzmann distribution (softmax) over the rank of each action, regulated by a temperature parameter τ. In other words, the neural network scores each possible actions, then they are ranked starting with the lowest value. Finally, we apply a softmax on these rank and draw from the resulting probability distribution."
  },
  {
    "objectID": "blog/posts/pedroai-rl/pedroai-rl.html#replay-buffer",
    "href": "blog/posts/pedroai-rl/pedroai-rl.html#replay-buffer",
    "title": "How PedroAI works",
    "section": "Replay buffer",
    "text": "Replay buffer\nThe replay buffer is the memory of the AI. Experiences (state, action, reward, next state) collected by the collectors are stored in this buffer. It enables “experience replay”. The learner sample the buffer randomly to build batches of experiences and “replays” them to train the neural network.\nPedroAI employ a Prioritized Experience Replay, meaning that each experience is associated with a priority. The priority value corresponds to the error the AI makes when it tries to score this experience. The more the AI is wrong about an experience, the more it is likely to sample this experience again, learn from it, and correct its estimation.\n\n\n\nReplay buffer with priorities"
  },
  {
    "objectID": "blog/posts/pedroai-rl/pedroai-rl.html#learner",
    "href": "blog/posts/pedroai-rl/pedroai-rl.html#learner",
    "title": "How PedroAI works",
    "section": "Learner",
    "text": "Learner\nIn the PedroAI project, the learner is independent of the collectors, this is a separate worker. Its loop looks like this:\n\n\n\nPedroAI learner\n\n\n\n\nSample the replay buffer to create a batch\n\n\nFit one batch to estimate the Q values\n\n\nCompare the Q values estimation with the target Q values to compute an error for each experience in the batch\n\n\nUpdate the neural network weights in order to produce smaller errors next time\n\n\nUpdate the priorities of the replay buffer using the error values\n\n\nAs we have seen in the previous article, PedroAI uses Deep Q-Learning. Its neural network estimates the expected return of a state for each action it could take.\nDuring step 3), the learner compares the neural network Q values estimates with target values. Target values are the true expected return values for each action (or rather an approximation of the true value, more on this later). Then, the learner adjusts the neural network weights to make the Q value estimation closer to the target values.\nThe difference between the estimation and target value, the error, is then used to update replay buffer priorities at step 5).\nIt is important to note that the neural network estimates the expected return directly and not only the reward between current and next state. It means that with one state, it estimates the expected discounted cumulative reward that takes into account all future rewards.\nThe neural network tries to estimates this value for each action possible:\n\nBut how can AI pull this trick and estimate the expected return directly with one state ? We will find out in the next chapter.\nBefore we move on, here is the complete diagram of PedroAI’s training loop:\n\n\n\nDetailed PedroAI training loop"
  },
  {
    "objectID": "blog/posts/pedroai-rl/pedroai-rl.html#trackmania-naive-reward-function",
    "href": "blog/posts/pedroai-rl/pedroai-rl.html#trackmania-naive-reward-function",
    "title": "How PedroAI works",
    "section": "Trackmania naive reward function",
    "text": "Trackmania naive reward function\nThe true objective of Trackmania is to complete a map in the minimum amount of time possible. A straightforward reward function would be to reward the AI when it finishes a map with a reward inversly proportional to the time it took.\nThe issue with this reward function is that it is way to sparse. At the start of training, the AI knows nothing about its environment and all actions are taken randomly. The AI will have to wander the map for ages before finishing the map even once and start learning something.\nVideo\nEven if the AI manages to finish a map regularly, it will be very diffcult for it to know what went well and what went wrong in the run. What turn it took well and what turn it took poorly ?\nWe have to define a better reward function for training."
  },
  {
    "objectID": "blog/posts/pedroai-rl/pedroai-rl.html#pedroai-reward-function",
    "href": "blog/posts/pedroai-rl/pedroai-rl.html#pedroai-reward-function",
    "title": "How PedroAI works",
    "section": "PedroAI reward function",
    "text": "PedroAI reward function\nPedroAI defines a reward function that gives feedback at each step. The challenge is to find a dense reward function that is a good proxy for the true objective “complete the map in the minimum amount of time”.\nUsing a reference trajectory (author or wr), the AI gets a positive reward for:\n\nmaking progress on the reference trajectory\ncompleting the map\n\nIt gets a negative reward for:\n\nbeing in a mistake state (too far from reference trajectory or not making progress)\nrespawning or ending the round without completing the map (happens when the AI is in a mistake state for too long)\n\nLet’s go into more detail on each of these points.\n\nProgress on reference trajectory\nProgress at each step is the length of completed trajectory between previous and current state. The car position is projected to the trajectory to compute this length.\n\nThen the AI gets rewarded as follow :\n\nIn other words, the AI is rewarded for the speed at which it completes the reference trajectory. If the AI would be following the reference trajectory perfectly at a constant speed this reward would simplify to :\n\nWill the AI be able to do better than the reference trajectory ? Yes, it will! Consider the following reference trajectory:\n\nThe AI will make more progess faster if it goes straight and doesn’t zigzag.\n\nEven a poor trajectory will help the AI learn to play well.\nFinally, the farther the car is from the trajectory, the more this reward is discounted. It helps the AI not wander too long on the wrong path. If the AI is closer than 10 meters, the reward is not discounted. After that, the reward is discounted linearly.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndiscount_threshold = 10\nmax_distance = 50\n\n\n@np.vectorize\ndef distance_discount(distance):\n    if distance &lt; discount_threshold:\n        return 1.0\n    elif max_distance &lt; distance:\n        return 0.0\n    return 1.0 - (distance - discount_threshold) / (max_distance - discount_threshold)\n\n\nx = np.linspace(0, max_distance * 1.1, 100)\nplt.xlabel(\"distance to reference trajectory (m)\")\nplt.ylabel(\"reward multiplier\")\nplt.plot(x, distance_discount(x))\nplt.show()\n\n\n\n\n\nFigure 1: Distance discount\n\n\n\n\n\n\nCompleting the map\nCompleting the map gives the AI a bigger reward. We have to make this reward big because the AI won’t get further reward after finishing the map. If this reward is too low, the risk is that during training, the AI prefer to miss the finish in the hope to get more reward later.\nPedroAI tackles this issue by giving the AI a reward equivalent to its expected return if it were to follow a reference trajectory perfectly at its current speed.\nRemember that the expected return, or expected cumulative reward is computed as follows:\n\nPedroAI’s formula for map completion reward is:\n\nThis way, the finish reward is almost always larger than what the AI might expect if it was still driving along a trajectory. Additionally, it incentivise the AI to cross the finish line with maximum speed.\nThis approach generally results in faster completion times, but there is a drawback in that the AI may sometimes choose longer routes to gain more speed before finishing.\n\n\nMistake state\nThe AI gets a small penalty for being in a mistake state. There are two possible mistakes:\n\nBeing too far from the trajectory: the distance between the car and his projection on the reference trajectory is more than 50 meters.\nNot making progress on the reference trajecory: the car already got farther on the trajectory before during the round.\n\nPedroAI gives a penalty of ≈14 points, which correspond to the amount of point the AI would get if it was driving at 50km/h following the reference trajectory. I tuned this value and found out that -14 yields the best results.\n\n\nRespawn or end round\nIf the AI is in a mistake state for more than 4 seconds, it respawns.\nFrom the point of vue of the AI, that is where the round ends; it doesn’t know that it is instead only respawning. For those familiar with reinforcement learning, this is the end of the episode, but for the in-game round and my satistics, this is only a respawn.\nIf the AI respawns 4 times on the same checkpoint and finds itself in a mistake state for more than 4 seconds again, it will end the round for good.\nSimilar to map completion reward, when the AI must respawn or end the round, PedroAI gives it the expected return if the AI where to stay in a mistake state indefinitely.\n\nNow, you should have a better understanding of how PedroAI works. In summary:\n\nPedroAI uses an adapted version of Deep Q-Learning with four seperate workers, three to collect experiences and one to learn from those experiences.\nThe neural network estimates the expected discounted cumulative rewards directly with one state. It doesn’t need to predict everything that will happen in the future. When learning, the trick is to reuse the neural network on the next state to estimate the future rewards.\nThe true Trackmania reward “complete the map in the minimum amount of time possible” is too sparse for reinforcement learning. PedroAI uses a denser reward function leveraging author or wr trajectory.\n\nStay tuned for the third blog post in this series where I present the technical details of the project: neural network architecture, frameworks, telemtry data gathering, and more … I am still working on it 🤓"
  },
  {
    "objectID": "blog/posts/rl-introduction/rl-introduction.html",
    "href": "blog/posts/rl-introduction/rl-introduction.html",
    "title": "Introduction to Deep Reinforcement Learning (Trackmania AI POV)",
    "section": "",
    "text": "Welcome to the wonderful land of Deep Reinforcement Learning, a hot topic in Artifical Intelligence.\nDeep RL is a type of Machine Learning where an agent learns how to behave in an environment by performing actions and looking at the results.\nIn this blog post, we will explore the basics of deep reinforcement learning. We will focus on what PedroAI project uses. The project features an AI learning to play the popular racing game Trackmania. The entire process is streamed on Twitch allowing viewers to watch the AI improve over time.\nThis article is the first of a two-part series where we first learn about deep reinforcement learning basics before diving deeper into the specificities of the PedroAI project.\nA third article about the technical details of PedroAI is being written as you read this."
  },
  {
    "objectID": "blog/posts/rl-introduction/rl-introduction.html#the-big-picture",
    "href": "blog/posts/rl-introduction/rl-introduction.html#the-big-picture",
    "title": "Introduction to Deep Reinforcement Learning (Trackmania AI POV)",
    "section": "The big picture",
    "text": "The big picture\nThe idea behind Reinforcement Learning is that an agent will learn from the environment by interacting with it through trial and error and receiving rewards, negative or positive, as feedback for performing actions.\nImagine putting your little sister in front of a slightly modified version of Trackmania where crossing a checkpoint or the finish line gives you one point but falling in the water substracts one point. You put a controller in her hands and let her alone.\n\nYour sister will interact with the environment, the Trackmania game, by pressing the forward trigger (action). She got a checkpoint, that’s a +1 reward. It’s positive, she just understood that in this game she must get checkpoints.\n\nShe continues to press forward but presses left too late to take the turn and falls in the water, that’s -1 reward.\n\nBy interacting with her environment, through trial and error, your little sister understood that, in this environment, she must take checkpoints and not fall.\nWithout any supervision, your sister will get better and better at playing the game.\nThat’s how humans and animals learn, through interaction. Reinforcement Learning is just a computational approach of learning from action."
  },
  {
    "objectID": "blog/posts/rl-introduction/rl-introduction.html#a-formal-definition",
    "href": "blog/posts/rl-introduction/rl-introduction.html#a-formal-definition",
    "title": "Introduction to Deep Reinforcement Learning (Trackmania AI POV)",
    "section": "A formal definition",
    "text": "A formal definition\nIf we take now a formal definition:\n\nReinforcement learning is a framework for solving control tasks, also called decision problems, by building agents that learn from the environment by interacting with it through trial and error and receiving rewards, positive or negative, as unique feedback.\n\nBut how does Reinforcement Learning work?"
  },
  {
    "objectID": "blog/posts/rl-introduction/rl-introduction.html#the-rl-process",
    "href": "blog/posts/rl-introduction/rl-introduction.html#the-rl-process",
    "title": "Introduction to Deep Reinforcement Learning (Trackmania AI POV)",
    "section": "The RL Process",
    "text": "The RL Process\nThe RL Process is a loop of state, action, reward and next state:\n\n\n\nThe RL Process loop\n\n\nTo understand the RL process, let’s consider PedroAI learning to play Trackmania:\n\n\n\nThe RL Process loop with Trackmania\n\n\n\nThe Agent receives state S0 from the Environment, the first frame of the game.\nBased on that state S0, the agent takes action A0, the car will go forward.\nThe Environment goes to a new state S1, a new frame.\nThe Environment gives some reward R1.\n\nThis RL loop outputs a sequence of state, action, reward and next state.\n\n\n\n\n\nThe agent’s goal is to maximize its cumulative reward, called return."
  },
  {
    "objectID": "blog/posts/rl-introduction/rl-introduction.html#the-reward-hypothesis-the-central-idea-of-reinforcement-learning",
    "href": "blog/posts/rl-introduction/rl-introduction.html#the-reward-hypothesis-the-central-idea-of-reinforcement-learning",
    "title": "Introduction to Deep Reinforcement Learning (Trackmania AI POV)",
    "section": "The reward hypothesis: the central idea of Reinforcement Learning",
    "text": "The reward hypothesis: the central idea of Reinforcement Learning\nReinforcement Learning is based on the reward hypothesis, which is that all goals can be described as the maximization of the return (cumulative reward). In Reinforcement Learning, to have the best behavior, we need to maximize the return."
  },
  {
    "objectID": "blog/posts/rl-introduction/rl-introduction.html#rewards-and-the-discounting",
    "href": "blog/posts/rl-introduction/rl-introduction.html#rewards-and-the-discounting",
    "title": "Introduction to Deep Reinforcement Learning (Trackmania AI POV)",
    "section": "Rewards and the discounting",
    "text": "Rewards and the discounting\nThe reward is fundamental in RL because it’s the only feedback the agent gets. It enables the agent to know if the action taken was good or not.\nThe cumulative reward at each time step t can be written as:\n\nWich is equivalent to:\n\nNow imagine you want to calculate the expected return of a state. You want to know the cumulative reward you can expect to get given your current state.\nYou could sum all the reward you expect to get in the future until the end of the run. However, in practice, the rewards that come sooner are more likely to happen since they are more predictable than the long-term future reward. That is why we introduce a discount rate for future potential rewards.\nLet’s say your agent is at the start of the following track. Your goal is to cross the maximum number of checkpoints before falling off the road.\n\nIt’s more probable to cross the checkpoints near your position than the ones close to the finish. The farther a checkpoint is from your position, the more likely it is to fall off before getting it.\nConsequently, the reward given for crossing a checkpoint far away is discounted more. We are not sure we’ll be able to cross the far away checkpoint.\nIn practice, to discount the rewards, we proceed like this:\n\nWe define a discount rate called gamma. It must be between 0 and 1. Most of the time between 0.99 and 0.90.\n\n\nThe larger the gamma, the smaller the discount. This means our agent cares more about the long-term reward.\nOn the other hand, the smaller the gamma, the bigger the discount. This means our agent cares more about the short term reward (the nearest checkpoints here).\n\n\nThen, each reward will be discounted by gamma to the exponent of the time step. As the time step increases, the car will have more opportunities to fall off, so the future reward is less and less likely to happen.\n\nThe discounted cumulative reward can be written as:\n\nOr:"
  },
  {
    "objectID": "blog/posts/rl-introduction/rl-introduction.html#explorationexploitation-tradeoff",
    "href": "blog/posts/rl-introduction/rl-introduction.html#explorationexploitation-tradeoff",
    "title": "Introduction to Deep Reinforcement Learning (Trackmania AI POV)",
    "section": "Exploration/Exploitation tradeoff",
    "text": "Exploration/Exploitation tradeoff\nBefore looking at how to solve Reinforcement Learning problems, we must cover a very important topic: the exploration/exploitation trade-off.\n\nExploration is exploring the environment by trying random actions in order to find more information about the environment.\nExploitation is exploiting known information to maximize the reward.\n\nRemember, the goal of our RL agent is to maximize the expected cumulative reward. However, we can fall into a common trap.\nLet’s take an example:\n\nHere our RL agent is rewarded for reaching the finish line fast. The faster it cross the finish line, the more reward it gets.\nNow consider that our RL agent learnt that going forward in a straight line is a good way to reach its destination if there is no obstacle in between. The agent will always take the road on the left and never try the road on the right even if the right road is actually faster (exploitation).\nBut if our agent does a little bit of exploration and try the road on the right, it will discover a faster route that yields a bigger reward (exploration).\nThis is what we call the exploration/exploitation trade-off. We need to balance how much we explore the environment and how much we exploit what we know about the environment.\nTherefore, we must define a rule that helps to handle this trade-off. We’ll see in future post how PedroAI handles this trade-off."
  },
  {
    "objectID": "blog/posts/rl-introduction/rl-introduction.html#how-to-solve-reinforcement-learning-problems",
    "href": "blog/posts/rl-introduction/rl-introduction.html#how-to-solve-reinforcement-learning-problems",
    "title": "Introduction to Deep Reinforcement Learning (Trackmania AI POV)",
    "section": "How to solve Reinforcement Learning problems",
    "text": "How to solve Reinforcement Learning problems\nIn other words, how to build an RL agent that can select the actions that maximize its expected cumulative reward?"
  },
  {
    "objectID": "blog/posts/rl-introduction/rl-introduction.html#the-policy-π-the-agents-brain",
    "href": "blog/posts/rl-introduction/rl-introduction.html#the-policy-π-the-agents-brain",
    "title": "Introduction to Deep Reinforcement Learning (Trackmania AI POV)",
    "section": "The Policy π: the agent’s brain",
    "text": "The Policy π: the agent’s brain\nThe Policy π is the brain of our agent. It’s the function that tells us what action to take given the state we are in. So, it defines the agent’s behavior at a given time.\n\n\n\nPolicy π\n\n\nThis policy is the function we want to learn. Our goal is to find the optimal policy π, the policy that maximizes expected return when the agent acts according to it. We find this π through training.\nThere are two approaches to train our agent to find this optimal policy π*:\n\nDirectly, by teaching the agent to learn which action to take given the state it is in: Policy-Based Methods. We will not discuss this method here, see https://huggingface.co/blog/deep-rl-intro#policy-based-methods for more information.\nIndirectly, teach the agent to learn which state is more valuable and then take the action that leads to the more valuable states: Value-Based Methods. This is the method PedroAI uses."
  },
  {
    "objectID": "blog/posts/rl-introduction/rl-introduction.html#value-based-methods",
    "href": "blog/posts/rl-introduction/rl-introduction.html#value-based-methods",
    "title": "Introduction to Deep Reinforcement Learning (Trackmania AI POV)",
    "section": "Value-based methods",
    "text": "Value-based methods\nIn value-based methods, instead of training a policy function, we train a value function that maps a state to the expected value of being at that state.\nThe value of a state is the expected discounted return the agent can get if it starts in that state, and then act according to our policy.\n“Act according to our policy” just means that our policy is “going to the state with the highest value”.\n\nHere we see that our value function defined value for each possible state.\n\nThanks to our value function, at each step our policy will select the state with the biggest value defined by the value function: -8, then -7, then -6 (and so on) to attain the goal."
  },
  {
    "objectID": "blog/posts/rl-introduction/rl-introduction.html#the-deep-in-reinforcement-learning",
    "href": "blog/posts/rl-introduction/rl-introduction.html#the-deep-in-reinforcement-learning",
    "title": "Introduction to Deep Reinforcement Learning (Trackmania AI POV)",
    "section": "The “Deep” in Reinforcement Learning",
    "text": "The “Deep” in Reinforcement Learning\n⇒ What we’ve talked about so far is Reinforcement Learning. But where does the “Deep” come into play?\nDeep Reinforcement Learning introduces deep neural networks to solve Reinforcement Learning problems — hence the name “deep”.\nFor instance, PedroAI uses Deep Q-Learning. It uses a Neural Network to approximate the expected value of a state for each action it could take.\n\n\n\nDeep Q learning for Trackmania\n\n\nIf you want to learn about deep learning in general, not only reinforcement learning, you should definitely watch the fastai Practical Deep Learning for Coders (Free).\nThat was a lot of information, if we summarize:\n\nReinforcement Learning is a computational approach of learning from action. We build an agent that learns from the environment by interacting with it through trial and error and receiving rewards, negative or positive, as feedback.\nThe goal of any RL agent is to maximize its expected cumulative reward (also called expected return) because RL is based on the reward hypothesis, which is that all goals can be described as the maximization of the expected cumulative reward.\nThe RL process is a loop that outputs a sequence of state, action, reward and next state.\nTo calculate the expected cumulative reward (expected return), we discount the rewards: the rewards that come sooner (at the beginning of the game) are more probable to happen since they are more predictable than the long-term future reward.\nTo solve an RL problem, you want to find an optimal policy, the policy is the “brain” of your AI that will tell us what action to take given a state. The optimal one is the one who gives you the actions that maximize the expected return.\nThere are two ways to find your optimal policy:\n\nBy training your policy directly: policy-based methods.\nBy training a value function that tells us the expected return the agent will get at each state and use this function to define our policy: value-based methods.\n\nFinally, we speak about Deep RL because we introduce deep neural networks to estimate the value of a state hence, the name “deep”.\n\nI encourage you to read the next blog post in this series How PedroAI works."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "PedroAI Blog",
    "section": "",
    "text": "How PedroAI works\n\n\n\n\n\n\n\n\n\n\n\n\nApr 26, 2023\n\n\nPierre Porcher\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Deep Reinforcement Learning (Trackmania AI POV)\n\n\n\n\n\n\n\n\n\n\n\n\nApr 25, 2023\n\n\nPierre Porcher\n\n\n\n\n\n\nNo matching items"
  }
]